{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSfqWRAWN4KJw+PwdQHIPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DkeH5u4pmpdm"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"da6333d6"},"source":["### ¿Cómo funciona un Árbol de Decisión (Decision Tree) para Regresión?\n","\n","Un Árbol de Decisión es un modelo no paramétrico supervisado que se utiliza tanto para clasificación como para regresión. Cuando se usa para regresión, el objetivo es predecir un valor de salida continuo (un número).\n","\n","Su funcionamiento se basa en dividir repetidamente el espacio de características en regiones más pequeñas y manejables, haciendo preguntas simples sobre las características de los datos. Imagina que es como un juego de '20 preguntas':\n","\n","1.  **Nodos Raíz y Nodos Internos**: Cada nodo en el árbol representa una 'pregunta' sobre una característica (ej. '¿La edad de la casa es mayor a 30 años?').\n","2.  **Ramas**: Las respuestas a estas preguntas dirigen a la observación por diferentes ramas del árbol.\n","3.  **Nodos Hoja (Leaf Nodes)**: Una vez que se llega a un nodo hoja, no hay más preguntas. La predicción para cualquier observación que caiga en este nodo hoja es típicamente el valor promedio de las variables objetivo de todas las muestras de entrenamiento que llegaron a ese mismo nodo.\n","4.  **Criterio de División**: El algoritmo elige las preguntas (divisiones) que mejor reducen la varianza (o el error cuadrático medio) en la variable objetivo dentro de cada una de las subregiones resultantes. El objetivo es crear nodos hojas donde los valores de `y` sean lo más homogéneos posible.\n","\n","Los árboles de decisión son muy intuitivos y fáciles de interpretar, pero pueden ser propensos al sobreajuste si son muy profundos y no se regulan correctamente (por ejemplo, mediante la poda o limitando su profundidad).\n","\n","### Python\n","\n","```python\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Instanciar el modelo con parámetros (ej. profundidad máxima)\n","model = DecisionTreeRegressor(max_depth=5, random_state=42)\n","```"]},{"cell_type":"markdown","metadata":{"id":"864f6ddd"},"source":["### Ejemplo de DecisionTreeRegressor con `fetch_california_housing`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a8cb9ac","executionInfo":{"status":"ok","timestamp":1771818879757,"user_tz":480,"elapsed":190,"user":{"displayName":"Luis Antonio García González","userId":"12491503091916673552"}},"outputId":"46b75736-b231-4a63-87d2-4e79dd3f471a"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor # Importamos DecisionTreeRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","\n","# 1. Cargar el dataset\n","california = fetch_california_housing(as_frame=True)\n","X = california.data\n","y = california.target\n","\n","# 2. Dividir los datos en conjuntos de entrenamiento y prueba\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n","print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras\")\n","\n","# 3. Instanciar y entrenar el modelo DecisionTreeRegressor\n","# Limitamos la profundidad para evitar el sobreajuste excesivo y mejorar la interpretabilidad\n","dt_model = DecisionTreeRegressor(max_depth=10, random_state=42) # Usamos max_depth para control\n","dt_model.fit(X_train, y_train)\n","\n","print(\"Modelo DecisionTreeRegressor entrenado exitosamente.\")\n","\n","# 4. Realizar predicciones en el conjunto de prueba\n","y_pred_dt = dt_model.predict(X_test)\n","\n","# 5. Calcular y mostrar métricas de evaluación para DecisionTreeRegressor\n","r2_dt = r2_score(y_test, y_pred_dt)\n","mae_dt = mean_absolute_error(y_test, y_pred_dt)\n","rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n","\n","print(f\"Métricas para DecisionTreeRegressor (max_depth=10):\")\n","print(f\"  R2 Score: {r2_dt:.4f}\")\n","print(f\"  Mean Absolute Error (MAE): {mae_dt:.4f}\")\n","print(f\"  Root Mean Squared Error (RMSE): {rmse_dt:.4f}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del conjunto de entrenamiento: 16512 muestras\n","Tamaño del conjunto de prueba: 4128 muestras\n","Modelo DecisionTreeRegressor entrenado exitosamente.\n","Métricas para DecisionTreeRegressor (max_depth=10):\n","  R2 Score: 0.6829\n","  Mean Absolute Error (MAE): 0.4332\n","  Root Mean Squared Error (RMSE): 0.6446\n"]}]}]}